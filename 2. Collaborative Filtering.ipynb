{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Recommender Systems - Collaborative Filtering in Python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to the design of recommender systems that has wide use is collaborative filtering.Collaborative filtering methods are based on collecting and analyzing a large amount of information on usersâ€™ behaviors, activities or preferences and predicting what users will like based on their similarity to other users. <br><br>\n",
    "A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an \"understanding\" of the item itself.Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past.<br><br>\n",
    "This notebook contains implementations of User-User and Item-Item collaborative Filtering algorithms for the Movie Lens dataset.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sortedcontainers import SortedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Original Dataset - https://www.kaggle.com/grouplens/movielens-20m-dataset<br>\n",
    "The original dataset has been proprocessed to filter out and keep only the top users movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## user_to_movie_map={}  ## Key:= User_id, Value:= [list of movies] \n",
    "## movie_to_user_map={}  ## Key:= Movie_id, Value:=[list of users] \n",
    "## train_ratings={}      ## Key:= (User_id, Movie_id) Value:=Rating \n",
    "## test_ratings={}       ## Key:= (User_id, Movie_id) Value:=Rating \n",
    "## user_statistics={}    ## Key:= User_id, Values:=(Avg_rating, Norm_of_deviations) \n",
    "## movie_statistics={}   ## Key:= Movie_id, Values:=(Avg_rating, Norm_of_deviations)\n",
    "\n",
    "with open('./data/user_to_movie_map.pkl', 'rb') as fp:\n",
    "    user_to_movie_map=pickle.load(fp)\n",
    "\n",
    "with open('./data/movie_to_user_map.pkl', 'rb') as fp:\n",
    "    movie_to_user_map=pickle.load(fp)\n",
    "\n",
    "with open('./data/train_ratings.pkl', 'rb') as fp:\n",
    "    train_ratings=pickle.load(fp)\n",
    "\n",
    "with open('./data/test_ratings.pkl', 'rb') as fp:\n",
    "    test_ratings=pickle.load(fp)\n",
    "\n",
    "with open('./data/user_statistics.pkl', 'rb') as fp:\n",
    "    user_statistics=pickle.load(fp)\n",
    "    \n",
    "with open('./data/movie_statistics.pkl', 'rb') as fp:\n",
    "    movie_statistics=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 300\n",
      "Number of unique movies: 800\n",
      "Total ratings in the dataset: 179576\n",
      "User-Item matrix size: 240000\n",
      "User-Item matrix empty percentage: 25.176666666666666\n"
     ]
    }
   ],
   "source": [
    "n_users=len(user_to_movie_map)\n",
    "n_movies=len(movie_to_user_map)\n",
    "matrix_size=n_users*n_movies\n",
    "n_ratings=len(train_ratings)+len(test_ratings)\n",
    "\n",
    "print(\"Number of unique users:\",n_users)\n",
    "print(\"Number of unique movies:\",n_movies)\n",
    "print(\"Total ratings in the dataset:\",n_ratings)\n",
    "\n",
    "print(\"User-Item matrix size:\",matrix_size)\n",
    "print(\"User-Item matrix empty percentage:\",(matrix_size-n_ratings)*100/(matrix_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User - User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_common_movies=5  ## For each user consider only other users with min_common_movies\n",
    "n_neighbours=25      ## For each user Consider n_neighbours with highest absolute weight similarity\n",
    "k=30                 ## Verbose printing for every kth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated neighbours for user:  0\n",
      "Calculated neighbours for user:  30\n",
      "Calculated neighbours for user:  60\n",
      "Calculated neighbours for user:  90\n",
      "Calculated neighbours for user:  120\n",
      "Calculated neighbours for user:  150\n",
      "Calculated neighbours for user:  180\n",
      "Calculated neighbours for user:  210\n",
      "Calculated neighbours for user:  240\n",
      "Calculated neighbours for user:  270\n"
     ]
    }
   ],
   "source": [
    "## Key: User_id, Value: List of (similarity,user_id) of max length n_neighbours \n",
    "## (Sorted by absolute similarity in decreasing order)\n",
    "user_neighbours={i:SortedList(key=lambda x: -abs(x[0])) for i in range(n_users)}\n",
    "\n",
    "for user1 in range(n_users):\n",
    "    for user2 in range(user1+1,n_users):\n",
    "        \n",
    "        ## Find the common movies between user1 and user2\n",
    "        common_movies=user_to_movie_map[user1].intersection(user_to_movie_map[user2])\n",
    "        \n",
    "        if len(common_movies) <= min_common_movies: continue\n",
    "        \n",
    "        u1_ratings = np.array( [ train_ratings[(user1,movie)] for movie in common_movies ] )\n",
    "        u2_ratings = np.array( [ train_ratings[(user2,movie)] for movie in common_movies ] )\n",
    "        \n",
    "        u1_avg, u1_norm = user_statistics[user1]\n",
    "        u2_avg, u2_norm = user_statistics[user2]\n",
    "        \n",
    "        numerator=sum((u1_ratings-u1_avg)*(u2_ratings-u2_avg))\n",
    "        similarity=numerator/(u1_norm*u2_norm)\n",
    "        \n",
    "        user_neighbours[user1].add((similarity,user2))\n",
    "        user_neighbours[user2].add((similarity,user1))\n",
    "        \n",
    "        ## If the lists contain more than the allowed users, pop items\n",
    "        if len(user_neighbours[user1])>n_neighbours:\n",
    "            user_neighbours[user1].pop()\n",
    "        if len(user_neighbours[user2])>n_neighbours:\n",
    "            user_neighbours[user2].pop()\n",
    "            \n",
    "    if user1%k==0: \n",
    "        print(\"Calculated neighbours for user: \",user1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_predict(user,movie):\n",
    "    \n",
    "    numerator=denominator=0\n",
    "\n",
    "    for similarity,user2 in user_neighbours[user]:\n",
    "        if (user2,movie) in train_ratings:\n",
    "            numerator+=similarity*(train_ratings[(user2,movie)]-user_statistics[user2][0])\n",
    "            denominator+=abs(similarity)\n",
    "            \n",
    "    pred=user_statistics[user][0]  ## Use average rating of the user\n",
    "    \n",
    "    if denominator>0:\n",
    "        pred+=(numerator/denominator) ## Adding the weighted deviations of neighbours\n",
    "        \n",
    "    pred=max(0.5,pred)\n",
    "    pred=min(5,pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item - Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_common_users=5   ## For each movie consider only other users with min_common_users\n",
    "n_neighbours=25      ## For each movie Consider n_neighbours with highest absolute weight similarity\n",
    "k=80                 ## Verbose printing for every kth iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated neighbours for movie:  0\n",
      "Calculated neighbours for movie:  80\n",
      "Calculated neighbours for movie:  160\n",
      "Calculated neighbours for movie:  240\n",
      "Calculated neighbours for movie:  320\n",
      "Calculated neighbours for movie:  400\n",
      "Calculated neighbours for movie:  480\n",
      "Calculated neighbours for movie:  560\n",
      "Calculated neighbours for movie:  640\n",
      "Calculated neighbours for movie:  720\n"
     ]
    }
   ],
   "source": [
    "## Key: movie_id, Value: List of (similarity,movie_id) of max length n_neighbours \n",
    "## (Sorted by absolute similarity in decreasing order)\n",
    "movie_neighbours={i:SortedList(key=lambda x: -abs(x[0])) for i in range(n_movies)}\n",
    "\n",
    "for movie1 in range(n_movies):\n",
    "    for movie2 in range(movie1+1,n_movies):\n",
    "        \n",
    "        ## Find the common users between movie1 and movie2\n",
    "        common_users=movie_to_user_map[movie1].intersection(movie_to_user_map[movie2])\n",
    "        \n",
    "        if len(common_users) <= min_common_users: continue\n",
    "        \n",
    "        m1_ratings = np.array( [ train_ratings[(user,movie1)] for user in common_users ] )\n",
    "        m2_ratings = np.array( [ train_ratings[(user,movie2)] for user in common_users ] )\n",
    "        \n",
    "        m1_avg, m1_norm = movie_statistics[movie1]\n",
    "        m2_avg, m2_norm = movie_statistics[movie2]\n",
    "        \n",
    "        numerator=sum((m1_ratings-m1_avg)*(m2_ratings-m2_avg))\n",
    "        similarity=numerator/(m1_norm*m2_norm)\n",
    "        \n",
    "        movie_neighbours[movie1].add((similarity,movie2))\n",
    "        movie_neighbours[movie2].add((similarity,movie1))\n",
    "        \n",
    "        ## If the lists contain more than the allowed movies, pop items\n",
    "        if len(movie_neighbours[movie1])>n_neighbours:\n",
    "            movie_neighbours[movie1].pop()\n",
    "        if len(movie_neighbours[movie2])>n_neighbours:\n",
    "            movie_neighbours[movie2].pop()\n",
    "            \n",
    "    if movie1%k==0: \n",
    "        print(\"Calculated neighbours for movie: \",movie1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_item_predict(user,movie):\n",
    "    \n",
    "    numerator=denominator=0\n",
    "\n",
    "    for similarity,movie2 in movie_neighbours[movie]:\n",
    "        if (user,movie2) in train_ratings:\n",
    "            numerator+=similarity*(train_ratings[(user,movie2)]-movie_statistics[movie2][0])\n",
    "            denominator+=abs(similarity)\n",
    "    \n",
    "    pred=movie_statistics[movie][0]  ## Use average rating of a movie\n",
    "    \n",
    "    if denominator>0:\n",
    "        pred+=(numerator/denominator) ## Average rating + weighted deviation of neighbours\n",
    "        \n",
    "    pred=max(0.5,pred)\n",
    "    pred=min(5,pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The mean squared error (MSE) is printed for train and test datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.8138737033960305\n",
      "Test error: 0.8205727179945912\n"
     ]
    }
   ],
   "source": [
    "train_errors=[(movie_statistics[m][0]-r)**2 for (u,m),r in train_ratings.items()]\n",
    "test_errors=[(movie_statistics[m][0]-r)**2 for (u,m),r in test_ratings.items()]\n",
    "\n",
    "print(\"Train error:\",np.mean(train_errors))\n",
    "print(\"Test error:\",np.mean(test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.5848141948510613\n",
      "Test error: 0.6216076548853222\n"
     ]
    }
   ],
   "source": [
    "train_errors=[(user_user_predict(u,m)-r)**2 for (u,m),r in train_ratings.items()]\n",
    "test_errors=[(user_user_predict(u,m)-r)**2 for (u,m),r in test_ratings.items()]\n",
    "\n",
    "print(\"Train error:\",np.mean(train_errors))\n",
    "print(\"Test error:\",np.mean(test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.42304816865788514\n",
      "Test error: 0.5642476318528216\n"
     ]
    }
   ],
   "source": [
    "train_errors=[(item_item_predict(u,m)-r)**2 for (u,m),r in train_ratings.items()]\n",
    "test_errors=[(item_item_predict(u,m)-r)**2 for (u,m),r in test_ratings.items()]\n",
    "\n",
    "print(\"Train error:\",np.mean(train_errors))\n",
    "print(\"Test error:\",np.mean(test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering approaches often suffer from three problems: cold start, scalability, and sparsity.<br><br>\n",
    "(i) Cold start: These systems often require a large amount of existing data on a user in order to make accurate recommendations.<br><br>\n",
    "(ii) Scalability: In many of the environments in which these systems make recommendations, there are millions of users and products. Thus, a large amount of computation power is often necessary to calculate recommendations.<br><br>\n",
    "(iii) Sparsity: The number of items sold on major e-commerce sites is extremely large. The most active users will only have rated a small subset of the overall database. Thus, even the most popular items have very few ratings.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://en.wikipedia.org/wiki/Recommender_system <br>\n",
    "2. https://www.kaggle.com/grouplens/movielens-20m-dataset <br>\n",
    "3. https://www.udemy.com/recommender-systems/ <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
