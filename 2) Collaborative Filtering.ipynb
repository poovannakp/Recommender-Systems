{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Recommender Systems - Collaborative Filtering in Python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to the design of recommender systems that has wide use is collaborative filtering.Collaborative filtering methods are based on collecting and analyzing a large amount of information on usersâ€™ behaviors, activities or preferences and predicting what users will like based on their similarity to other users. <br><br>\n",
    "A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an \"understanding\" of the item itself.Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past.<br><br>\n",
    "This notebook contains implementations of User-User and Item-Item collaborative Filtering algorithms.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load the data</a></span></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Exploration</a></span></li><li><span><a href=\"#User---User-Collaborative-Filtering\" data-toc-modified-id=\"User---User-Collaborative-Filtering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>User - User Collaborative Filtering</a></span></li><li><span><a href=\"#Item---Item-Collaborative-Filtering\" data-toc-modified-id=\"Item---Item-Collaborative-Filtering-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Item - Item Collaborative Filtering</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Baseline</a></span></li><li><span><a href=\"#User-User-Collaborative-Filtering\" data-toc-modified-id=\"User-User-Collaborative-Filtering-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>User-User Collaborative Filtering</a></span></li><li><span><a href=\"#Item-Item-Collaborative-Filtering\" data-toc-modified-id=\"Item-Item-Collaborative-Filtering-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Item-Item Collaborative Filtering</a></span></li></ul></li><li><span><a href=\"#Drawbacks-of-Collaborative-Filtering\" data-toc-modified-id=\"Drawbacks-of-Collaborative-Filtering-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Drawbacks of Collaborative Filtering</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sortedcontainers import SortedList\n",
    "\n",
    "FILE_PATH = \"./data/Movielens/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "The original dataset has been proprocessed to filter out and keep only the top users and items.<br>\n",
    "Please refer to the preprocessing notebook in the repo for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## user_to_item_map={}  ## Key:= User_id, Value:= [list of items] \n",
    "## item_to_user_map={}  ## Key:= item_id, Value:=[list of users] \n",
    "## train_ratings={}     ## Key:= (User_id, item_id) Value:=Rating \n",
    "## test_ratings={}      ## Key:= (User_id, item_id) Value:=Rating\n",
    "## user_statistics={}   ## Key:= User_id, Value:=(Avg_rating, Norm_of_deviations)\n",
    "## item_statistics={}   ## Key:= User_id, Value:=(Avg_rating, Norm_of_deviations)\n",
    "\n",
    "with open(FILE_PATH+'user_to_item_map.pkl', 'rb') as fp:\n",
    "    user_to_item_map=pickle.load(fp)\n",
    "\n",
    "with open(FILE_PATH+'item_to_user_map.pkl', 'rb') as fp:\n",
    "    item_to_user_map=pickle.load(fp)\n",
    "\n",
    "with open(FILE_PATH+'train_ratings.pkl', 'rb') as fp:\n",
    "    train_ratings=pickle.load(fp)\n",
    "\n",
    "with open(FILE_PATH+'test_ratings.pkl', 'rb') as fp:\n",
    "    test_ratings=pickle.load(fp)\n",
    "\n",
    "with open(FILE_PATH+'user_statistics.pkl', 'rb') as fp:\n",
    "    user_statistics=pickle.load(fp)\n",
    "    \n",
    "with open(FILE_PATH+'item_statistics.pkl', 'rb') as fp:\n",
    "    item_statistics=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 200\n",
      "Number of unique items: 600\n",
      "Total ratings in the dataset: 95871\n",
      "User-Item matrix size: 120000\n",
      "User-Item matrix empty percentage: 20.1075\n"
     ]
    }
   ],
   "source": [
    "n_users=len(user_to_item_map)\n",
    "n_items=len(item_to_user_map)\n",
    "matrix_size=n_users*n_items\n",
    "n_ratings=len(train_ratings)+len(test_ratings)\n",
    "\n",
    "print(\"Number of unique users:\",n_users)\n",
    "print(\"Number of unique items:\",n_items)\n",
    "print(\"Total ratings in the dataset:\",n_ratings)\n",
    "\n",
    "print(\"User-Item matrix size:\",matrix_size)\n",
    "print(\"User-Item matrix empty percentage:\",(matrix_size-n_ratings)*100/(matrix_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User - User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_user_neighbours(n_neighbours,min_common_items,k):\n",
    "    ## Key: User_id, Value: List of (similarity,user_id) of max length n_neighbours \n",
    "    ## (Sorted by absolute similarity in decreasing order)\n",
    "    user_neighbours={i:SortedList(key=lambda x: -abs(x[0])) for i in range(n_users)}\n",
    "\n",
    "    for user1 in range(n_users):\n",
    "        for user2 in range(user1+1,n_users):\n",
    "\n",
    "            ## Find the common items between user1 and user2\n",
    "            common_items=user_to_item_map[user1].intersection(user_to_item_map[user2])\n",
    "\n",
    "            if len(common_items) <= min_common_items: continue\n",
    "\n",
    "            u1_ratings = np.array( [ train_ratings[(user1,item)] for item in common_items ] )\n",
    "            u2_ratings = np.array( [ train_ratings[(user2,item)] for item in common_items ] )\n",
    "\n",
    "            u1_avg, u1_norm = user_statistics[user1]\n",
    "            u2_avg, u2_norm = user_statistics[user2]\n",
    "\n",
    "            numerator=sum((u1_ratings-u1_avg)*(u2_ratings-u2_avg))\n",
    "            similarity=numerator/(u1_norm*u2_norm)\n",
    "\n",
    "            user_neighbours[user1].add((similarity,user2))\n",
    "            user_neighbours[user2].add((similarity,user1))\n",
    "\n",
    "            ## If the lists contain more than the allowed users, pop items\n",
    "            if len(user_neighbours[user1])>n_neighbours:\n",
    "                user_neighbours[user1].pop()\n",
    "            if len(user_neighbours[user2])>n_neighbours:\n",
    "                user_neighbours[user2].pop()\n",
    "\n",
    "        if user1%k==0: \n",
    "            print(\"Calculated neighbours for user: \",user1)\n",
    "    return user_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_predict(user,item,user_neighbours):\n",
    "    \n",
    "    numerator=denominator=0\n",
    "\n",
    "    for similarity,user2 in user_neighbours[user]:\n",
    "        if (user2,item) in train_ratings:\n",
    "            numerator+=similarity*(train_ratings[(user2,item)]-user_statistics[user2][0])\n",
    "            denominator+=abs(similarity)\n",
    "            \n",
    "    pred=user_statistics[user][0]  ## Use average rating of the user\n",
    "    \n",
    "    if denominator>0:\n",
    "        pred+=(numerator/denominator) ## Adding the weighted deviations of neighbours\n",
    "        \n",
    "    pred=max(0.5,pred)\n",
    "    pred=min(5,pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item - Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_item_neighbours(n_neighbours,min_common_users,k):\n",
    "    ## Key: item_id, Value: List of (similarity,item_id) of max length n_neighbours \n",
    "    ## (Sorted by absolute similarity in decreasing order)\n",
    "    item_neighbours={i:SortedList(key=lambda x: -abs(x[0])) for i in range(n_items)}\n",
    "\n",
    "    for item1 in range(n_items):\n",
    "        for item2 in range(item1+1,n_items):\n",
    "\n",
    "            ## Find the common users between item1 and item2\n",
    "            common_users=item_to_user_map[item1].intersection(item_to_user_map[item2])\n",
    "\n",
    "            if len(common_users) <= min_common_users: continue\n",
    "\n",
    "            m1_ratings = np.array( [ train_ratings[(user,item1)] for user in common_users ] )\n",
    "            m2_ratings = np.array( [ train_ratings[(user,item2)] for user in common_users ] )\n",
    "\n",
    "            m1_avg, m1_norm = item_statistics[item1]\n",
    "            m2_avg, m2_norm = item_statistics[item2]\n",
    "\n",
    "            numerator=sum((m1_ratings-m1_avg)*(m2_ratings-m2_avg))\n",
    "            similarity=numerator/(m1_norm*m2_norm)\n",
    "\n",
    "            item_neighbours[item1].add((similarity,item2))\n",
    "            item_neighbours[item2].add((similarity,item1))\n",
    "\n",
    "            ## If the lists contain more than the allowed items, pop items\n",
    "            if len(item_neighbours[item1])>n_neighbours:\n",
    "                item_neighbours[item1].pop()\n",
    "            if len(item_neighbours[item2])>n_neighbours:\n",
    "                item_neighbours[item2].pop()\n",
    "\n",
    "        if item1%k==0: \n",
    "            print(\"Calculated neighbours for item: \",item1)\n",
    "            \n",
    "    return item_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_item_predict(user,item,item_neighbours):\n",
    "    \n",
    "    numerator=denominator=0\n",
    "\n",
    "    for similarity,item2 in item_neighbours[item]:\n",
    "        if (user,item2) in train_ratings:\n",
    "            numerator+=similarity*(train_ratings[(user,item2)]-item_statistics[item2][0])\n",
    "            denominator+=abs(similarity)\n",
    "    \n",
    "    pred=item_statistics[item][0]  ## Use average rating of a item\n",
    "    \n",
    "    if denominator>0:\n",
    "        pred+=(numerator/denominator) ## Average rating + weighted deviation of neighbours\n",
    "        \n",
    "    pred=max(0.5,pred)\n",
    "    pred=min(5,pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The mean squared error (MSE) is printed for train and test datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.8238406990373789\n",
      "Test error: 0.8135730078827685\n"
     ]
    }
   ],
   "source": [
    "train_errors=[(item_statistics[m][0]-r)**2 for (u,m),r in train_ratings.items()]\n",
    "test_errors=[(item_statistics[m][0]-r)**2 for (u,m),r in test_ratings.items()]\n",
    "\n",
    "print(\"Train error:\",np.mean(train_errors))\n",
    "print(\"Test error:\",np.mean(test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated neighbours for user:  0\n",
      "Calculated neighbours for user:  100\n"
     ]
    }
   ],
   "source": [
    "min_common_items=5   ## For each user consider only other users with min_common_items\n",
    "n_neighbours=25      ## For each user Consider n_neighbours with highest absolute weight similarity\n",
    "k=100                 ## Verbose printing for every kth iteration\n",
    "user_neighbours=initialize_user_neighbours(n_neighbours,min_common_items,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.5895553103600147\n",
      "Test error: 0.6194113057221694\n"
     ]
    }
   ],
   "source": [
    "train_errors=[(user_user_predict(u,m,user_neighbours)-r)**2 for (u,m),r in train_ratings.items()]\n",
    "test_errors=[(user_user_predict(u,m,user_neighbours)-r)**2 for (u,m),r in test_ratings.items()]\n",
    "\n",
    "print(\"Train error:\",np.mean(train_errors))\n",
    "print(\"Test error:\",np.mean(test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated neighbours for item:  0\n",
      "Calculated neighbours for item:  80\n",
      "Calculated neighbours for item:  160\n",
      "Calculated neighbours for item:  240\n",
      "Calculated neighbours for item:  320\n",
      "Calculated neighbours for item:  400\n",
      "Calculated neighbours for item:  480\n",
      "Calculated neighbours for item:  560\n"
     ]
    }
   ],
   "source": [
    "min_common_users=5   ## For each item consider only other users with min_common_users\n",
    "n_neighbours=25      ## For each item Consider n_neighbours with highest absolute weight similarity\n",
    "k=80                 ## Verbose printing for every kth iteration\n",
    "item_neighbours=initialize_item_neighbours(n_neighbours,min_common_users,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.4051205463827412\n",
      "Test error: 0.5708141774284932\n"
     ]
    }
   ],
   "source": [
    "train_errors=[(item_item_predict(u,m,item_neighbours)-r)**2 for (u,m),r in train_ratings.items()]\n",
    "test_errors=[(item_item_predict(u,m,item_neighbours)-r)**2 for (u,m),r in test_ratings.items()]\n",
    "\n",
    "print(\"Train error:\",np.mean(train_errors))\n",
    "print(\"Test error:\",np.mean(test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering approaches often suffer from three problems: cold start, scalability, and sparsity.<br><br>\n",
    "(i) Cold start: These systems often require a large amount of existing data on a user in order to make accurate recommendations.<br><br>\n",
    "(ii) Scalability: In many of the environments in which these systems make recommendations, there are millions of users and products. Thus, a large amount of computation power is often necessary to calculate recommendations.<br><br>\n",
    "(iii) Sparsity: The number of items sold on major e-commerce sites is extremely large. The most active users will only have rated a small subset of the overall database. Thus, even the most popular items have very few ratings.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://en.wikipedia.org/wiki/Recommender_system <br>\n",
    "2. https://www.kaggle.com/grouplens/movielens-20m-dataset <br>\n",
    "3. https://www.udemy.com/recommender-systems/ <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
